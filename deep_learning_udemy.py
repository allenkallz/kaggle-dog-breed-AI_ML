# -*- coding: utf-8 -*-
"""Deep_learning_udemy.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1IBIaNAvt3b8aLswvw-TtieNpCbWcs8Mx
"""

# Unzip 
#!unzip "drive/MyDrive/Colab Notebooks/udemy_dog_data/dog-breed-identification.zip" -d "drive/MyDrive/Colab Notebooks/udemy_dog_data/"

"""# Get Our workspace ready"""

# import TensorFlow in Colab
import tensorflow as tf
print("Tf Version is : ", tf.__version__)

# Import necessary tools
import tensorflow_hub as hub
print("Hub version : ", hub.__version__)

# check for GPU availability 
print("GPU", "available" if tf.config.list_physical_devices("GPU") else "NOT")

# import panads
import pandas as pd

"""# Load the data"""

## Getting our data ready (turning into Tensors)

# truning images into Tensors (numrical form as all machine algo accept only num data)

# checkout labels of our data

import pandas as pd

labels_csv = pd.read_csv("drive/MyDrive/Colab Notebooks/udemy_dog_data/labels.csv")
print(labels_csv.describe())
print(labels_csv.head())

# How many images of all breads

labels_csv["breed"].value_counts()

labels_csv["breed"].value_counts().plot.bar(figsize=(20,20))

labels_csv.breed.value_counts().median()

# Let`s view an image

from IPython.display import Image

Image("drive/MyDrive/Colab Notebooks/udemy_dog_data/train/000bec180eb18c7604dcecc8fe0dba07.jpg")

"""# Getting images and theri labels

let`s get a list of full images file pathnames.
"""

labels_csv.head()

filesnames = ["drive/MyDrive/Colab Notebooks/udemy_dog_data/train/" + fname + ".jpg" for fname in labels_csv.id]

Image(filesnames[9000])

labels = labels_csv["breed"]

import numpy as np

labels = np.array(labels)

len(labels)

# check for empty data

# convert breed into string 

unique_breeds = np.unique(labels)
len(unique_breeds)

# Turn every label into a boolean array 
boolean_labels = [label == unique_breeds for label in labels]

boolean_labels[0]

print(labels[0])
print(np.where(unique_breeds == labels[0]))

print(boolean_labels[0].argmax())
print(boolean_labels[0].astype(int))

"""### Creating traning and validation set
# kaggle only provide training set.
"""

# setup x and y
X = filesnames
Y = boolean_labels

NUM_IMAGES = 1000 #@param {type:"slider", min:1000, max: 5000}

# let split data 

from sklearn.model_selection import train_test_split

np.random.seed(42)

X_train, X_val, Y_train, Y_val = train_test_split(X[:NUM_IMAGES], Y[:NUM_IMAGES], test_size=0.2, random_state=42)

len(X_train), len(Y_train), len(X_val), len(Y_val)

from matplotlib.pyplot import imread

image = imread(filesnames[42])

image.shape

image.max(), image.min()

image

tf.constant(image)

IMG_SIZE = 224

# create a function for preprocessing the images

def process_image(image_path):
  """
    convert image into tensor
  """
  image = tf.io.read_file(image_path)
  
  # turn jpg into num tensore within 3 color channel RGB
  image = tf.image.decode_jpeg(image, channels=3)
  
  # covert the colour channel values from 0-255 to 0-1 values
  # Normalize our image
  image = tf.image.convert_image_dtype(image, tf.float32)

  # resize the image to (224,224)
  image = tf.image.resize(image, size=[IMG_SIZE, IMG_SIZE])
  
  return image

"""# Turining our data into batches  ... its good for data in batch

Why turn our data into batches?

loading and training 10,0000+ images in one go they might not fit into memory

so we plan 32 images at a time as a batch 

convert data into for train (image,label)
"""

# function to return tuple of (image,label)

def get_image_label(image_path, label):
  """
    
  """
  image = process_image(image_path)
  return image, label

# Define batch with 32 as var

BATCH_SIZE = 32

# create function to create batches
def create_data_batches(x, y=None, batch_size=BATCH_SIZE, valid_data=False, test_data=False):
  """
    Create batches of data out of image (x) and label y pairs

    shuffles the if it`s training data but dosen`t shuffle if it`s validation data
    also accept test data as input (no labels)

  """
  # if data is test_data don`t have labels
  if test_data:
    print("creating test data batches")
    data = tf.data.Dataset.from_tensor_slices((tf.constant(x)))
    data_batch  = data.map(process_image).batch(BATCH_SIZE)
    return data_batch
  
  # if data is valid data set we don`t need to shuffle it
  elif valid_data:
    print("creating valid data batches")
    data = tf.data.Dataset.from_tensor_slices((tf.constant(x),tf.constant(y)))
    # create image,label tuple and process image 
    data_batch = data.map(get_image_label).batch(BATCH_SIZE)
    return data_batch

  else:
    print("Creating the training data batches")
    
    data = tf.data.Dataset.from_tensor_slices((tf.constant(x), tf.constant(y)))

    # shuffle the data 
    data = data.shuffle(buffer_size=len(x))

    # create image,label tuple and process image 
    data = data.map(get_image_label)
    data_batch = data.batch(BATCH_SIZE)
    return data_batch

train_data = create_data_batches(X_train, Y_train)

val_data = create_data_batches(X_val, Y_val, valid_data=True)

# Checkout diff attribute of data batches

train_data.element_spec, val_data.element_spec

"""### visual data batches"""

import matplotlib.pyplot as plt

def show_25_images(images, labels):
  """
    Display a plot of 25 images and their labels from a data batch
  """
  plt.figure(figsize=(10,10))
  # loop for 25 images

  for i in range(25):
    # create subplot 5 row 5 column
    ax = plt.subplot(5, 5, i +1)
  
    # Display image
    plt.imshow(images[i])
    # add image label as title
    plt.title(unique_breeds[labels[i].argmax()])

    # turn the grid line off
    plt.axis("off")

train_images, train_labels = next(train_data.as_numpy_iterator())

 len(train_images), len(train_labels)

# now visualize the data into training batch
show_25_images(train_images, train_labels)

val_images , val_labels = next(val_data.as_numpy_iterator())
show_25_images(val_images, val_labels)

"""## Building model 
* The input shape (our images shape in the form of tensors) for our model
* output shape image
* URL of the model we want to use (existing model)

"""

# Setup input shape to the model

INPUT_SHAPE = [None, IMG_SIZE, IMG_SIZE, 3]  # batch height width colour

# output shape
OUTPUT_SHAPE = len(unique_breeds)

# setup model url from tensor flow hub

MODEL_URL = "https://tfhub.dev/google/imagenet/mobilenet_v2_130_224/classification/4"
# MODEL_URL = "https://tfhub.dev/deepmind/local-linearity/imagenet/1"

"""# Use kecan deep learning model"""

# Sequential api and functional api

# create function which build keras model

def create_model(input_shape=INPUT_SHAPE, output_shape = OUTPUT_SHAPE, model_url=MODEL_URL):

  print(f"building model with url : {model_url}")

  # setup the model
  model = tf.keras.Sequential([
                               hub.KerasLayer(model_url), 
                               tf.keras.layers.Dense(units=output_shape, 
                                                     activation="softmax")
                               ])

  # complile the model
  model.compile(
      loss=tf.keras.losses.CategoricalCrossentropy(), 
      optimizer=tf.keras.optimizers.Adam(), metrics=["accuracy"]
      )

  # build the model
  model.build(input_shape=input_shape)

  return model

model = create_model()

model.summary()

"""## creating callbacks

callbacks are helper function duning training to do save its progress.

Two callbacks

* Tensor board which helps track our  odel progress
* early stop training in case it take time 
"""

# Commented out IPython magic to ensure Python compatibility.
# load Tensorboard notebook extension
# %load_ext tensorboard

import datetime
import os
# create a function to build the tensorboard callback

def  create_tenssoorboard_callback():
  """
    create a log directory for storing tensorboard logs
  """
  logdir = os.path.join("drive/MyDrive/Colab Notebooks/udemy_dog_data/log",datetime.datetime.now().strftime("%Y%m%d-%H%M%S"))
  
  return tf.keras.callbacks.TensorBoard(log_dir=logdir)

"""### Early Stopping callback

early stopping help the model from overfitting by stopping training if a  certian evaluation metric stop improving.


"""

# create early stopping callback

early_stopping = tf.keras.callbacks.EarlyStopping(monitor="val_accuracy", patience=3)

"""## Train a model

* first train only on 1000 for save time and it`s working

"""

NUM_EPOCHS = 100 #@param {type:"slider" , min:10, max:100, step:10}

"""# Lets create a model that train the model

* create a model using the `create_model`
* setup a tensor broad callback using `create_tenssoorboard_callback`
* call `fii` function on our model passing it the training data , validation data number of epochs to train for `NUM_EPOCHS` and the callback we like to use
"""

## bulid the function to train and return a train model

def train_model():
  """
    Train  a given model and returns the trained version
  """
  # create a model
  moodel = create_model()

  # Create new tensorboard  callback
  tensorboard = create_tenssoorboard_callback()

  # fit the model to data passing it callback we created
  model.fit(x=train_data, 
            epochs=NUM_EPOCHS,
            validation_data= val_data,
            validation_freq = 1, 
            callbacks=[tensorboard, early_stopping])
  
  return model

# fit the model to data

model = train_model()

"""### Check the tensorboard log

the tensorboard magic function %tensoorboard will access the logs directory we created earlier and visualize its contents
"""

# Commented out IPython magic to ensure Python compatibility.
# %tensorboard --logdir drive/MyDrive/Colab\ Notebooks/udemy_dog_data/log

"""## Making and evaluating predictions using a trained model"""

val_data

# make prediction
predictions = model.predict(val_data, verbose=1)
predictions

predictions.shape

predictions[113].argmax()

unique_breeds[113]

def get_pred_label(prediction_probabilities):
  """
    Turn prediciton prob into labels
  """
  return  unique_breeds[np.argmax(prediction_probabilities)]

# Get a predicted label based on an array of prediction probabilities
pred_label = get_pred_label(predictions[1])
pred_label

"""Now val data in batch dataset now unbatch to make prediction on imagess and compare it with val label """

#  create a function to unbatch a batch dataset

images_ = []
labels_ = []

# loop through unbatch data
for img, lab in val_data.unbatch().as_numpy_iterator():
  images_.append(img)
  labels_.append(lab)

images_[0]

labels_[0]

get_pred_label(labels_[0])

get_pred_label(predictions[0])

def unbatchify(data):
  """
    take batch data set of image, label and return seperate arrays of images and labels
  """
  img = []
  lab = []

  for i, l in data.unbatch().as_numpy_iterator():
    img.append(i)
    lab.append(unique_breeds[l.argmax()])
  
  return img, lab


# unbatch val data

val_images, val_labels = unbatchify(val_data)

"""# Now we have

* prediction label
* validation label
* validation images

Let`s have some function to make all these a bit more visualize




"""

def plot_pred(prediction_probabilities, labels, images, n=1):
  """
    view the prediction , ground trunth and images for sample n
  """

  pred_prob  , true_label, image = prediction_probabilities[n], labels[n], images[n]

  # get the pred label

  pred_label = get_pred_label(pred_prob)

  # plot image
  plt.imshow(image)

  plt.xticks([])
  plt.yticks([])

  # change color on basis of correct or wrong

  if pred_label == true_label:
    color = "green"
  else:
    color = "red"

  # change plot title
  plt.title("{} {:2.0f}% {}".format(
      pred_label, 
      np.max(pred_prob) * 100,
       true_label), color=color)

plot_pred(prediction_probabilities=predictions, labels=val_labels, images=val_images)

plot_pred(prediction_probabilities=predictions, labels=val_labels, images=val_images, n=42)

plot_pred(prediction_probabilities=predictions, labels=val_labels, images=val_images , n=77)

"""## View top 10 predictions"""

def plot_pred_conf(prediction_probabilities, labels, n=1):
  """
    
  """

  pred_prob, true_label = prediction_probabilities[n], labels[n]

  # get prediction label
  pred_label = get_pred_label(pred_prob)

  top_10_pred_indexes = pred_prob.argsort()[-10:][::-1]

  top_10_pred_values = pred_prob[top_10_pred_indexes]

  #find top 10 pred labels
  top_10_pred_labels = unique_breeds[top_10_pred_indexes]

  # setup plot

  top_plot = plt.bar(np.arange(len(top_10_pred_labels)),
                     top_10_pred_values, color="grey")
  
  plt.xticks(np.arange(len(top_10_pred_labels)), 
                        labels=top_10_pred_labels, 
                        rotation="vertical")

  # change the color of true label
  if np.isin(true_label, top_10_pred_labels):
    top_plot[np.argmax(top_10_pred_labels == true_label)].set_color("green")
  
  else:
    pass

plot_pred_conf(prediction_probabilities=predictions, labels=val_labels,n=9)

plot_pred_conf(prediction_probabilities=predictions, labels=val_labels,n=10)

i_multiplier = 10
num_rows = 3
num_cols = 2
num_images = num_rows * num_cols

plt.figure(figsize=(10*num_cols, 5*num_rows))

for i in range(num_images):
  plt.subplot(num_rows, 2*num_cols, 2*i+1)
  plot_pred(prediction_probabilities=predictions, 
            labels=val_labels, 
            images=val_images, 
            n = i+i_multiplier)
  plt.subplot(num_rows, 2*num_cols, 2*i+2)
  plot_pred_conf(predictions, 
                 val_labels, n=i+i_multiplier)
  
plt.tight_layout(h_pad=1.0)
plt.show()

"""# Create confusion matrix

"""

# save and reload a trained model

def save_model(model, suffix=None):
  """
  """
  # create model dir pathname with current_time
  modeldir = os.path.join("drive/MyDrive/Colab Notebooks/udemy_dog_data/model", 
                          datetime.datetime.now().strftime("%Y%m%d-%H%M%s"))
  
  model_path = modeldir + "-" + suffix + ".h5"

  print(f"saving model to path {model_path}")

  model.save(model_path)

  return model_path

def load_model(model_path):
  """
    load a saved model from path
  """
  print(f"loading saved model from {model_path}")

  model = tf.keras.models.load_model(model_path,
                                     custom_objects = {"KerasLayer": hub.KerasLayer})
  
  return model

# save model of 1000 images

save_model(model, suffix="1000-images-mobilenetv2-Adam")

# test load model

loaded_1000_image_model = load_model("drive/MyDrive/Colab Notebooks/udemy_dog_data/model/20210320-10201616235657-1000-images-mobilenetv2-Adam.h5")

# eval pre save model

model.evaluate(val_data)

loaded_1000_image_model.evaluate(val_data)

"""## Training a big dog model on large image data set"""

len(X), len(Y)

len(X_train)

# Create a data batch with full dataset

full_data = create_data_batches(X, Y)

full_data

len(full_data)

full_model = create_model()

# create full model callback

full_model_tensorboard = create_tenssoorboard_callback()

# No validation set when training on all the data so we can`t monitor validation accuracy

full_model_early_stopping = tf.keras.callbacks.EarlyStopping(monitor="accuracy", patience=3)

# fit the full model

# full_model.fit(x=full_data, 
#                epochs=NUM_EPOCHS,
#                callbacks=[full_model_tensorboard, full_model_early_stopping])

# save_model(full_model, suffix="full-images-set-mobilenetv2A-Adam")

loaded_full_model = load_model("drive/MyDrive/Colab Notebooks/udemy_dog_data/model/20210320-11231616239429-full-images-set-mobilenetv2A-Adam.h5")

len(X)

# Load the test data
TEST_DATA_DIR = "/content/drive/MyDrive/Colab Notebooks/udemy_dog_data/test"

# LOAD TEST IMAGE FILE NAME

test_path = "drive/MyDrive/Colab Notebooks/udemy_dog_data/test/"

test_filenames = [test_path + fname for fname in os.listdir(test_path)]

test_filenames[:5], len(test_filenames)

# CONVERT TEST DATA INTO DATA_BATCHES 
test_data = create_data_batches(test_filenames, test_data=True)

len(test_data)

test_data

test_predictions = loaded_full_model.predict(test_data, verbose=1)

"""# Save test_predictions

 np.savetxt(f_path, test_predictions, delimiter=",")
"""

# save test predictions
# np.savetxt("drive/MyDrive/Colab Notebooks/udemy_dog_data/preds_array.csv", test_predictions, delimiter=",")

# Load the test predictions
# load_test_prediction = np.loadtxt("drive/MyDrive/Colab Notebooks/udemy_dog_data/preds_array.csv", delimiter=",")

## Prepare test prediction for test submission on kaggle

# create panads dataframe

preds_df = pd.DataFrame(columns=["id"]+ list(unique_breeds))

# append test image id to preds_dataframe
test_ids = [os.path.splitext(path)[0] for path in os.listdir(test_path)]


preds_df["id"] = test_ids

# add prediction prob to each dog breed column
preds_df[list(unique_breeds)] = test_predictions


# save to csv
preds_df.to_csv("drive/MyDrive/Colab Notebooks/udemy_dog_data/full_model_preds.csv", index=False)

"""# Making prediction on image 
write function for make prediction on custom image


"""

def custom_pred_on_image(dir_path):
  """
  """

  custom_image_path = [os.path.join(dir_path, fname) for fname in os.listdir(dir_path)]
  
  # data batchs
  custom_batch_data = create_data_batches(custom_image_path, test_data=True)

  # make prediction on custom data
  custom_preds = loaded_full_model.predict(custom_data)

  # convert into label
  custom_preds_label = [get_pred_label(custom_preds[i]) for i in range(len(custom_preds))]

  return custom_preds, custom_preds_label


# plot custom preds
# plt.figure(figsize=(10,10))

